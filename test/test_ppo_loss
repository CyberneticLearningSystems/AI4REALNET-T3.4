import torch
from torch import nn
from src.controllers.PPOController import PPOController
from src.algorithms.loss import policy_loss, value_loss

def test_ppo_loss_update():
    torch.manual_seed(42)

    # --- Setup a small fake environment ---
    n_agents = 2
    state_size = 4
    n_actions = 3
    batch_size = 5

    # Fake states and next states
    states = torch.randn(batch_size, state_size)
    next_states = torch.randn(batch_size, state_size)

    # Fake actions, rewards, log_probs, values
    actions = torch.randint(0, n_actions, (batch_size,))
    old_log_probs = torch.randn(batch_size)
    gaes = torch.randn(batch_size)
    state_values = torch.randn(batch_size)
    next_state_values = torch.randn(batch_size)
    rewards = torch.randn(batch_size)
    dones = torch.zeros(batch_size)

    # --- Setup controller and optimizer ---
    controller = PPOController(state_size=state_size, action_size=n_actions)
    optimizer = torch.optim.Adam(controller.get_parameters(), lr=1e-3)

    # Forward pass
    logits = controller.actor_network(states)
    action_dist = torch.distributions.Categorical(logits=logits)
    log_probs = action_dist.log_prob(actions)
    entropy = action_dist.entropy()
    values = controller.critic_network(states).squeeze(-1)
    next_values = controller.critic_network(next_states).squeeze(-1)

    # --- Compute losses ---
    actor_loss = policy_loss(gae=gaes, new_log_prob=log_probs, old_log_prob=old_log_probs, clip_eps=0.2)
    value_targets = gaes + state_values
    critic_loss = value_loss(predicted_values=values, target_values=value_targets)
    total_loss = actor_loss + critic_loss

    # --- Backprop ---
    optimizer.zero_grad()
    total_loss.backward()

    # Check that gradients exist
    actor_grads = [p.grad for p in controller.actor_network.parameters()]
    critic_grads = [p.grad for p in controller.critic_network.parameters()]
    assert all(g is not None for g in actor_grads), "Actor gradients are missing"
    assert all(g is not None for g in critic_grads), "Critic gradients are missing"

    # --- Optimizer step ---
    before_params = [p.clone() for p in controller.actor_network.parameters()]
    optimizer.step()
    after_params = [p for p in controller.actor_network.parameters()]

    # Check that parameters changed
    param_deltas = [torch.sum(torch.abs(a - b)) for a, b in zip(before_params, after_params)]
    assert any(delta.item() > 0 for delta in param_deltas), "Actor parameters did not update"

    print("PPO loss and optimizer test passed.")
